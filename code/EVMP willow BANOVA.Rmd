---
title: "EVMP BANOVA"
author: ""
date: ""
output: 
  html_document: 
    theme: journal
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
opts_knit$set(root.dir=normalizePath('../')) # this is required if Rmd is nested below the project directory
opts_chunk$set(fig.path = "../output/figures/") # corrected path and added dev. Needed to specify a subdirectory for figs
```

```{r}
set.seed(123)
```


```{r,echo=FALSE}
# library(here)
# here()
# install.packages("bindrcpp")
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(fs))
suppressPackageStartupMessages(library(sf))
# library(raster)
suppressPackageStartupMessages(library(janitor))
suppressPackageStartupMessages(library(readxl))
# library(glue)
suppressPackageStartupMessages(library(mapview))
# library(ggmap)
# library(ggrepel)
suppressPackageStartupMessages(library(viridis))
# library(ggExtra)
suppressPackageStartupMessages(library(ggstance))
suppressPackageStartupMessages(library(readxl))
suppressPackageStartupMessages(library(DT))
# library(kableExtra)
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(skimr)) 
suppressPackageStartupMessages(library(dataMaid))
# library(ggmap)
library(lubridate)
library(mapview)
library(vtree)

library(emmeans)
## bayesian
library(rstanarm)
library(bayestestR)
# library(easystats)
# install.packages("devtools")
# devtools::install_github("easystats/report")

library(rstatix) # rstatix: Pipe-Friendly Framework for Basic Statistical Tests

# devtools::install_github("dkahle/ggmap")
library(ggmap)
library(gt)
library(ggrepel)

# install.packages("devtools")
# devtools::install_github("easystats/easystats")

## vroom https://vroom.r-lib.org/articles/vroom.html
```

## Functions


```{r, eval = TRUE}
## function to describe model out
desc.post1 <- function(model){
  describe_posterior(
  model,
  effects = "fixed",
  component = "all",
  ci_method = "hdi",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>% 
    as_tibble() %>% 
    DT::datatable(caption = "Posterior description")
}

```


```{r, eval=FALSE, echo=FALSE}
# A Bayesian analysis returns a posterior distribution for each parameter (or effect). To minimally describe these distributions. Report the following:
# 
# *a point-estimate of centrality  
# *information characterizing the estimation uncertainty (the dispersion).  
# * Additionally, one can also report indices of effect existence and/or significance.
# 
# Suggest reporting the median as an index of centrality, as it is more robust compared to the mean or the MAP estimate. However, in case of severly skewed posterior distributions, the MAP estimate could be a good alternative
# 
# The 89% Credible Interval (CI) appears as a reasonable range to characterize the uncertainty related to the estimation, being more stable than higher thresholds (such as 90% and 95%). We also recommend computing the CI based on the HDI rather than quantiles, favouring probable, - over central - values.
# 
# The rstanarm package automates several data preprocessing steps making its use very similar to that of lme4 in the following way.
# 
# Missing Data - rstanarm automatically discards observations with NA values for any variable used in the model.
# 
# Identifiers - rstanarm does not require identifiers to be sequential. It is good practice for all cluster and unit identifiers, as well as categorical variables be stored as factors. This applies to using lme4 as much as it does to rstanarm. One can check the structure of the variables by using the str() function.
```


## Willow height - macroplot

### Core winter range

### Read in and munge for modeling

```{r}
willow <- read_csv("./output/exported_data/willow_mcro.csv")

willow <- willow %>% 
  select(-c(pType, WILDERNESS, contains("UTM"))) 

willow.ht <- willow %>% 
  filter(!is.na(PLANT_HT_CM)) %>% 
  # distinct(yr)
  mutate(timeClass = case_when(yr == 2008 ~ "BL",
                              yr == 2009 ~ "BL", 
                              TRUE ~ as.character(yr))) 

willow.ht <- willow.ht %>% 
  filter(timeClass == "BL" | timeClass == "2013" | timeClass == "2018") %>%
  mutate(timeClass = as.factor(timeClass)) %>% 
  mutate(timeClass = fct_relevel(timeClass, "2018", "2013", "BL")) %>% 
  mutate(FENCED = as.factor(FENCED))

# willow.ht %>% distinct(timeClass)

willow.ht <- willow.ht %>% 
  janitor::clean_names()

## filter out just the willow species
willow.ht <- willow.ht %>% 
  filter(genus == "willow")

willow.ht %>% 
  # distinct(fenced)
  glimpse()

```


```{r}

willow.ht.wc <- willow.ht %>% 
  filter(site_type == "WC") %>% 
  dplyr::select(-c(range_type,burned,removed,valley, valley_full))

willow.ht.wc %>% 
  glimpse()

```



#### Traditional AOV

```{r}
aov_model_wc <- aov(plant_ht_cm ~ time_class*fenced + Error(site_id/(time_class*fenced)), data = willow.ht.wc)


#### example
# stress.aov <- with(myData.mean,
#                    aov(stress ~ music * image +
#                        Error(PID / (music * image)))
# )
# Error() term we threw in there? Pretty simple: what weâ€™re saying is that we want to look at how stress changes as a function of the music and image that participants were shown. (Thus the stress ~ music * image) The asterisk specifies that we want to look at the interaction between the two IVs as well. But since this was a repeated measures design, we need to specify an error term that accounts for natural variation from participant to participant. We do this with the Error() function: specifically, we are saying that we want to control for that between-plot variation over all of our within-subjects variables.


```

### Bayesian Analyses

#### weekly informative priors, repeated measures
Fit model - STAN_LMER


```{r}
stmod_ht1 <- stan_lmer(plant_ht_cm ~ time_class*fenced + (1 | site_id), data = willow.ht.wc,
                      iter = 5000)

prior_summary(stmod_ht1)
```

```{r, eval=FALSE}
# ### model 2 - priors from web
# Working with errors
## guess at some more informative priors
stmod_ht2 <- stan_lmer(plant_ht_cm ~ time_class*fenced + (1 | site_id), data = willow.ht.wc, iter = 6000,
                       prior = cauchy(0,c(0.707,0.707,0.5)),          # as per Rouder et al., 2012
                        prior_intercept = student_t(3,0,10),          # weakly informative
                        prior_aux = exponential(.1),                  # weakly informative
                        prior_covariance = decov(1,1,1,1))            # weakly informative


```

#### Model 2 - flat priors, repeated measures

```{r}
# null prior
stmod_ht1_noPrior <- stan_lmer(plant_ht_cm ~ time_class*fenced + (1 | site_id), data = willow.ht.wc, prior = NULL, iter = 8000)

```

```{r, eval = FALSE}
# note the function in the next chunk is good
describe_posterior(
  stmod_ht1,
  effects = "all",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
)
# Posterior: point estimates
centrality <- point_estimate(stmod_ht1)  # Get indices of centrality
centrality
```

### model checks

```{r}
# Compare the model with default weak priors and the one with the null
pp_check(stmod_ht1)
# pp_check(stmod_ht2)
pp_check(stmod_ht1_noPrior)


```

get posteriors

```{r}
posteriors <- insight::get_parameters(stmod_ht1)
posteriors2 <- insight::get_parameters(stmod_ht1_noPrior)

names(posteriors)
# head(posteriors) %>% 
#   datatable()# Show the first 6 rows
# glimpse(posteriors)

posteriors.t1 <- posteriors %>%
  # names()
  pivot_longer(cols = starts_with("time"), names_to = "variable", values_to = "values")


library(ggridges)

posteriors.t1 %>% 
  ggplot() +
  ggridges::geom_density_ridges(aes(x = values, y = variable))

posteriors.t1 %>% 
  ggplot() +
  geom_density(aes(fill = variable, x = values, color = variable), alpha = .12) +
  theme_minimal()

```

# shinystan
```{r, eval = FALSE}

## shinystan
# install.packages("shinystan")
library("shinystan")
# launch_shinystan_demo()
# More info

## shiny eval
launch_shinystan(stmod_ht1)

```

Use the emmeans package to explore and extract marginal effects and estimates from our fitted model. 
We can also estimate (based on posterior draws) the difference between the two simple effects for timeclass between the fenced:

```{r}
library(emmeans)
pairs.timeClass <- pairs(emmeans(stmod_ht1, ~ time_class))
pairs.timeClass
plot(pairs.timeClass) +
  theme_minimal() +
  labs(title = "Point estimate displayed: median 
HPD interval probability: 0.95")
```

```{r}
library(emmeans)
pairs.fenced <- pairs(emmeans(stmod_ht1, ~ fenced))
pairs.fenced
plot(pairs.fenced) +
  theme_minimal() +
  labs(title = "Point estimate displayed: median 
HPD interval probability: 0.95")
```


```{r, eval = FALSE}

pairs.timeClassxfenced <- emmeans(stmod_ht1, ~time_class * fenced)
pairs(pairs.timeClassxfenced, by = "time_class") # simple effects for color
plot(pairs.timeClassxfenced) +
  theme_minimal() +
  labs(x = "Median estimate", y = "", caption = "pairs.timeClassxRange") 


```

```{r}
pairs.fencedxtimeClass <- emmeans(stmod_ht1, ~fenced * time_class)
pairs(pairs.fencedxtimeClass, by = "time_class") # simple effects for color
plot(pairs.fencedxtimeClass) +
  theme_minimal() +
  labs(x = "Median estimate", y = "", caption = "pairs.fencedxtimeClass") 


pairs(pairs.fencedxtimeClass, by = "fenced")
plot(pairs.fencedxtimeClass, by = "fenced") +
  theme_minimal() +
  labs(x = "Median estimate", y = "", caption = "pairs.fencedxtimeClass") 

```


Compute Confidence/Credible/Compatibility Intervals (CI) or Support Intervals (SI) for Bayesian and frequentist models. 

bayestestR provides two methods to compute credible intervals, the Highest Density Interval (HDI) (hdi()) and the Equal-tailed Interval (ETI) (eti()). These methods can also be changed via the method argument of the ci() function.

```{r}
# Compute HDI and ETI
ci_hdi <- ci(posteriors, method = "HDI")
ci_eti <- ci(posteriors, method = "ETI")

# Plot the distribution and add the limits of the two CIs
# posteriors %>% 
#   select(timeClass2:RANGE_TYPE2) %>% 
#   estimate_density(extend=TRUE) %>% 
#   ggplot(aes(x=x, y=y)) +
#   geom_area(fill="orange") +
#   theme_classic() +
#   # HDI in blue
#   geom_vline(xintercept=ci_hdi$CI_low, color="royalblue", size=3) +
#   geom_vline(xintercept=ci_hdi$CI_high, color="royalblue", size=3) +
#   # Quantile in red
#   geom_vline(xintercept=ci_eti$CI_low, color="red", size=1) +
#   geom_vline(xintercept=ci_eti$CI_high, color="red", size=1)

```


```{r}
# c_color_shape_interaction <- contrast(stan_model, interaction = c("pairwise","pairwise"))
# c_color_shape_interaction
```


#### stan_aov

```{r, eval=FALSE}

# my_prior <- normal(location = c(-10, 0), scale = c(5, 2), autoscale = FALSE)
# stan_glm(y ~ x1 + x2, data = dat, prior = my_prior)

```



```{r, eval = FALSE}
# Using the fantastic emmeans package, we can explore and extract marginal effects and estimates from our fitted model. For example, we can estimate the main effect for color:
# 

stav <- stan_aov(stemDen.ha ~ RANGE_TYPE*timeClass, data = ht_perc1, prior = NULL)

library(emmeans)
pairs.timeClass <- pairs(emmeans(stav, ~ timeClass))
pairs.timeClass

pairs.range_type <- pairs(emmeans(stav, ~ RANGE_TYPE))
pairs.range_type


plot(pairs.timeClass)
plot(pairs.range_type)


# We can also estimate (based on posterior draws) the difference between the two simple effects for color between the levels of shape:

em.timeClassxRange <- emmeans(stav, ~timeClass * RANGE_TYPE)
pairs(em.timeClassxRange, by = "timeClass") # simple effects for color
plot(em.timeClassxRange)

```

>EMMs for later factor levels are subtracted from those for earlier levels; if you want the comparisons to go in the other direction, use pairs(pigs.emm.s, reverse = TRUE). Also, in multi-factor situations, you may specify by factor(s) to perform the comparisons separately at the levels of those factors.

#### Describe posterior

```{r}
# Compute indices
pd <- p_direction(stmod_ht1)
percentage_in_rope <- rope(stmod_ht1, ci=1)

# Visualise the pd
plot(pd)
```


### Scrap

#### Repeated measures

```{r, echo=FALSE, eval=FALSE}
# worked R bloggers example
# see: https://www.r-bloggers.com/two-way-anova-with-repeated-measures/

set.seed(5250)

myData <- data.frame(PID = rep(seq(from = 1,
                               to = 50, by = 1), 20),
                     stress = sample(x = 1:100,
                                     size = 1000,
                                     replace = TRUE),
                     image = sample(c("Happy", "Angry"),
                                    size = 1000,
                                    replace = TRUE),
                     music = sample(c("Disney", "Horror"),
                                    size = 1000,
                                    replace = TRUE)
)

myData <- within(myData, {
  PID   <- factor(PID)
  image <- factor(image)
  music <- factor(music)
})

myData <- myData[order(myData$PID), ]
head(myData)

PID stress image  music
  1     90 Happy Disney
  1     70 Angry Horror
  1     61 Angry Horror
  1     87 Happy Horror
  1     79 Happy Disney
  1     95 Happy Horror
So we see that we have one row per observation per participant. If your dataset is in wide form rather than long, Iâ€™d suggest checking out our article on converting between wide and long since everything from this point out assumes that your data look like whatâ€™s shown above!

Extracting Condition Means
Before we can run our ANOVA, we need to find the mean stress value for each participant for each combination of conditions. Weâ€™ll do that with:

myData.mean <- aggregate(myData$stress,
                      by = list(myData$PID, myData$music,
                              myData$image),
                      FUN = 'mean')

colnames(myData.mean) <- c("PID","music","image","stress")

myData.mean <- myData.mean[order(myData.mean$PID), ]
head(myData.mean)

PID  music   image   stress
  1 Disney   Angry 39.33333
  1 Horror   Angry 65.50000
  1 Disney   Happy 68.00000
  1 Horror   Happy 69.57143
  1 Disney Neutral 40.00000
  1 Horror Neutral 52.66667
So now weâ€™ve gone from one row per participant per observation to one row per participant per condition. At this point weâ€™re ready to actually construct our ANOVA!

Building the ANOVA
Now, our actual ANOVA is going to look something like this:

stress.aov <- with(myData.mean,
                   aov(stress ~ music * image +
                       Error(PID / (music * image)))
)
But whatâ€™s all that mean? Whatâ€™s with that funky Error() term we threw in there? Pretty simple: what weâ€™re saying is that we want to look at how stress changes as a function of the music and image that participants were shown. (Thus the stress ~ music * image) The asterisk specifies that we want to look at the interaction between the two IVs as well. But since this was a repeated measures design, we need to specify an error term that accounts for natural variation from participant to participant. (E.g., I might react a little differently to scary music than you do because I love zombie movies and you hate them!) We do this with the Error() function: specifically, we are saying that we want to control for that between-participant variation over all of our within-subjects variables.

Now that weâ€™ve specified our model, we can go ahead and look at the results:

summary(stress.aov)

Error: PID
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals 49   8344   170.3               

Error: PID:music
          Df Sum Sq Mean Sq F value Pr(>F)
music      1      1    0.78   0.003  0.954
Residuals 49  11524  235.19               

Error: PID:image
          Df Sum Sq Mean Sq F value Pr(>F)
image      1     61   61.11   0.296  0.589
Residuals 49  10127  206.66               

Error: PID:music:image
            Df Sum Sq Mean Sq F value Pr(>F)
music:image  1    564   563.8   2.626  0.112
Residuals   49  10520   214.7  
We see that there is no main effect of either music:

F(1, 49) = 0.003; p-value = 0.954
or image:

F(1, 49) = 0.296; p-value = 0.589
on participant stress. Likewise, we see that there is not a significant interaction effect between the two independent variables:

F(1, 49) = 2.626; p-value = 0.112
```


```{r}

```

